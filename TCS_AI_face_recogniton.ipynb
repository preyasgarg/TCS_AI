{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TCS_AI_face_recogniton.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZcZFJqel30z",
        "colab_type": "text"
      },
      "source": [
        "# Step 1 - Import the dataset\n",
        "\n",
        "- Upload the dataset file to google drive.\n",
        "- Copy the dataset for google drive to loacal directory.\n",
        "- Parse the JSON file to reveal the data.\n",
        "- Download the images from the link provided in the json file.\n",
        "- Save the images into appropriate directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I13Mlbxy73wb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "936ecf94-01f0-4b2c-88b1-38f88b3e36a7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW1R_uMR75bV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp 'drive/My Drive/TCS_HUMAN_AIN/Face_Recognition.json' '.'\n",
        "!cp 'drive/My Drive/TCS_HUMAN_AIN/Indian_Number_plates.json' '.'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgTC_cQb8WnW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "data = []\n",
        "with open('Face_Recognition.json') as f:\n",
        "  for line in f:\n",
        "    data.append(json.loads(line))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNFBnWL082Tr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.mkdir('faces')\n",
        "os.mkdir('images')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9CABLUfC5WS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "temp = 0\n",
        "for line in data:\n",
        "  f = open('images/'+'%04d'%temp + '.jpg','wb')\n",
        "  f.write(requests.get(line['content']).content)\n",
        "  f.close()\n",
        "  temp+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Mr9PsnMh88J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzNJ_ZMtm10P",
        "colab_type": "text"
      },
      "source": [
        "# Step 2 - First part of the network (face Detector)\n",
        "\n",
        "- As this is **Pseudo Code so mathias detector**.\n",
        "- Leave appropriate margin around face while cropping.\n",
        "- Later in the final Implementation a better face detector will be used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Snf8le_Nh81e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#instead of this mathias Detector, FaceNet will be used to acheive higher detecitons\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import dlib\n",
        "import numpy as np\n",
        "import argparse\n",
        "from keras.utils.data_utils import get_file\n",
        "import shutil\n",
        "\n",
        "if os.path.isdir('mathais_face_detected'):\n",
        "  shutil.rmtree('mathais_face_detected')\n",
        "os.mkdir('mathais_face_detected')\n",
        "\n",
        "\n",
        "def main():\n",
        "    depth = 16\n",
        "    k = 8\n",
        "    margin = 0.4\n",
        "\n",
        "    # for face detection\n",
        "    detector = dlib.get_frontal_face_detector()\n",
        "\n",
        "    # load model and weights\n",
        "    img_size = 256\n",
        "    temp = os.listdir(\"images\")\n",
        "    temp.sort()\n",
        "    for fimg in temp:\n",
        "        print(fimg)\n",
        "        img = cv2.imread(\"images/\"+fimg)\n",
        "        if not(img is None):\n",
        "          input_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "          img_h, img_w, _ = np.shape(input_img)\n",
        "\n",
        "          # detect faces using dlib detector\n",
        "          detected = detector(input_img, 1)\n",
        "          faces = np.empty((len(detected), img_size, img_size, 3))\n",
        "\n",
        "          if len(detected) > 0:\n",
        "              for i, d in enumerate(detected):\n",
        "                  x1, y1, x2, y2, w, h = d.left(), d.top(), d.right() + 1, d.bottom() + 1, d.width(), d.height()\n",
        "                  xw1 = max(int(x1 - margin * w), 0)\n",
        "                  yw1 = max(int(y1 - margin * h), 0)\n",
        "                  xw2 = min(int(x2 + margin * w), img_w - 1)\n",
        "                  yw2 = min(int(y2 + margin * h), img_h - 1)\n",
        "                  faces[i, :, :, :] = cv2.resize(img[yw1:yw2 + 1, xw1:xw2 + 1, :], (img_size, img_size))\n",
        "                  cv2.imwrite(\"mathais_face_detected/\"+str(i)+'_'+fimg, faces[i, :, :, :])\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJkOu4201s1I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHEVvC8V2jcc",
        "colab_type": "text"
      },
      "source": [
        "# Step 3.1 - First(Pseudo) part of the network (face Detector)\n",
        "\n",
        "- As this i just a pseuode implementation so i used labels to crop out face for training other network.\n",
        "- Leave appropriate margin around face while cropping.\n",
        "- Later in the final Implementation a better face detector will be used.\n",
        "- Convert the dataset of labels into pandas dtaframe for easy use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gkoc3ETL1sx1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMAGE_SIZE = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUsMUVN41suA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not (os.path.isdir('faces_bo')):\n",
        "  os.mkdir('faces_bo')\n",
        "images = os.listdir('images')\n",
        "images.sort()\n",
        "import cv2\n",
        "temp=0\n",
        "datasheet = []\n",
        "for i in range(len(images)):\n",
        "  im = cv2.imread('images/'+images[i])\n",
        "  if not (im is None) and images[i] != '0047.jpg':\n",
        "    for j in data[i]['annotation']:\n",
        "      x1 = int(j['points'][0]['x'] * j['imageWidth'])\n",
        "      y1 = int(j['points'][0]['y'] * j['imageHeight'])\n",
        "      x2 = int(j['points'][1]['x'] * j['imageWidth'])\n",
        "      y2 = int(j['points'][1]['y'] * j['imageHeight'])\n",
        "      x_bo = int((x2-x1)*0.2)\n",
        "      y_bo = int((y2-y1)*0.2)\n",
        "      if (x1-x_bo)>0:\n",
        "        x1 = x1-x_bo\n",
        "      else:\n",
        "        x1=0\n",
        "      if (y1-y_bo)>0:\n",
        "        y1 = y1-y_bo\n",
        "      else:\n",
        "        y1=0\n",
        "      if (x2+x_bo)<j['imageWidth']:\n",
        "        x2 = x2+x_bo\n",
        "      else:\n",
        "        x2=j['imageWidth']\n",
        "      if (y2+y_bo)<j['imageHeight']:\n",
        "        y2 = y2+y_bo\n",
        "      else:\n",
        "        y2=j['imageHeight']\n",
        "      face_im = im[y1:y2,x1:x2]\n",
        "      temp_list = j['label'][:]\n",
        "      temp_list.sort()\n",
        "      if len(temp_list)>0 and temp_list[0].startswith('Emo'):\n",
        "        temp_list.insert(0,'Ethinicity_unknown')\n",
        "        temp_list.insert(0,'Age_unknown')\n",
        "      if 'Not_Face' in temp_list:\n",
        "        temp_list = ['Not_Face'] #remeber to use 28(face_image)\n",
        "      if len(temp_list) == 5: #69th image have a label with 2 ethinicity's\n",
        "        face_im = cv2.resize(face_im,(IMAGE_SIZE,IMAGE_SIZE))\n",
        "        cv2.imwrite('faces_bo/'+'%04d'%temp + '.jpg',face_im)\n",
        "        temp_list1 = temp_list[:2]+temp_list[3:]\n",
        "        temp_list1.insert(0,'faces_bo/'+'%04d'%temp + '.jpg')\n",
        "        temp_list1.insert(0,'%04d'%temp + '.jpg')\n",
        "        temp_list1.insert(0,images[i])\n",
        "        datasheet.append(temp_list1)\n",
        "        temp+=1\n",
        "        cv2.imwrite('faces_bo/'+'%04d'%temp + '.jpg',face_im)\n",
        "        temp_list2 = temp_list[:1]+temp_list[2:]\n",
        "        temp_list2.insert(0,'faces_bo/'+'%04d'%temp + '.jpg')\n",
        "        temp_list2.insert(0,'%04d'%temp + '.jpg')\n",
        "        temp_list2.insert(0,images[i])\n",
        "        datasheet.append(temp_list2)\n",
        "        temp+=1\n",
        "      else:\n",
        "        try:\n",
        "          face_im = cv2.resize(face_im,(IMAGE_SIZE,IMAGE_SIZE)) # one face image can't be resized in 25th image\n",
        "        except:\n",
        "          continue\n",
        "        cv2.imwrite('faces_bo/'+'%04d'%temp + '.jpg',face_im)\n",
        "        temp_list.insert(0,'faces_bo/'+'%04d'%temp + '.jpg')\n",
        "        temp_list.insert(0,'%04d'%temp + '.jpg')\n",
        "        temp_list.insert(0,images[i])\n",
        "        datasheet.append(temp_list)\n",
        "        temp+=1\n",
        "  else:\n",
        "    print('image not read is', images[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33aGJByI3KTH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df_bo = pd.DataFrame(datasheet,columns=['orig_im','face_image', 'path', 'age', 'ethinicity', 'emotion',  'gender'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVfFQmZT3K1f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_bo['emotion'].unique()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8W1OpmM3-WM",
        "colab_type": "text"
      },
      "source": [
        "# Step 4 - Data segmentaion and Custom Augmentation for Gender Classification.\n",
        "\n",
        "- As this i just a pseuode implementation so only trained for gender classification.\n",
        "- At later stage of full implementation ethnicities, age and emotion will also be trained.\n",
        "- Python and OpenCV is used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeVItAUU3k6N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.mkdir('gender_bo')\n",
        "os.mkdir('gender_bo/male')\n",
        "os.mkdir('gender_bo/female')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WfcBd043p2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shutil\n",
        "orig_lis = list(df_bo.orig_im)\n",
        "face_lis = list(df_bo.face_image)\n",
        "age_lis = list(df_bo.age)\n",
        "gender_lis = list(df_bo.gender)\n",
        "path_lis = list(df_bo.path)\n",
        "male = 0\n",
        "female = 0\n",
        "for i in range(len(gender_lis)):\n",
        "  #print(gender_lis[i])\n",
        "  if gender_lis[i] == 'G_Male':\n",
        "    shutil.copy2(path_lis[i],'gender_bo/male')\n",
        "    #print(gender_lis[i])\n",
        "    male +=1\n",
        "  elif gender_lis[i] == 'G_ Female':\n",
        "    shutil.copy2(path_lis[i],'gender_bo/female')\n",
        "    #print(gender_lis[i])\n",
        "    female +=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIx6sNqN4lHP",
        "colab_type": "text"
      },
      "source": [
        "# Step 4.1 - Data scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQME6jtPFV6i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "def central_scale_images(X_imgs, scales):\n",
        "    # Various settings needed for Tensorflow operation\n",
        "    boxes = np.zeros((len(scales), 4), dtype = np.float32)\n",
        "    for index, scale in enumerate(scales):\n",
        "        x1 = y1 = 0.5 - 0.5 * scale # To scale centrally\n",
        "        x2 = y2 = 0.5 + 0.5 * scale\n",
        "        boxes[index] = np.array([y1, x1, y2, x2], dtype = np.float32)\n",
        "    box_ind = np.zeros((len(scales)), dtype = np.int32)\n",
        "    crop_size = np.array([IMAGE_SIZE, IMAGE_SIZE], dtype = np.int32)\n",
        "    \n",
        "    X_scale_data = []\n",
        "    tf.reset_default_graph()\n",
        "    X = tf.placeholder(tf.float32, shape = (1, IMAGE_SIZE, IMAGE_SIZE, 3))\n",
        "    # Define Tensorflow operation for all scales but only one base image at a time\n",
        "    tf_img = tf.image.crop_and_resize(X, boxes, box_ind, crop_size)\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        \n",
        "        for img_data in X_imgs:\n",
        "            batch_img = np.expand_dims(img_data, axis = 0)\n",
        "            scaled_imgs = sess.run(tf_img, feed_dict = {X: batch_img})\n",
        "            X_scale_data.extend(scaled_imgs)\n",
        "    \n",
        "    X_scale_data = np.array(X_scale_data, dtype = np.float32)\n",
        "    return X_scale_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrhYAI6l4xE2",
        "colab_type": "text"
      },
      "source": [
        "# Step 4.2 - Data Translation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5fIW8kx36w0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from math import ceil, floor\n",
        "\n",
        "def get_translate_parameters(index):\n",
        "    if index == 0: # Translate left 20 percent\n",
        "        offset = np.array([0.0, 0.2], dtype = np.float32)\n",
        "        size = np.array([IMAGE_SIZE, ceil(0.8 * IMAGE_SIZE)], dtype = np.int32)\n",
        "        w_start = 0\n",
        "        w_end = int(ceil(0.8 * IMAGE_SIZE))\n",
        "        h_start = 0\n",
        "        h_end = IMAGE_SIZE\n",
        "    elif index == 1: # Translate right 20 percent\n",
        "        offset = np.array([0.0, -0.2], dtype = np.float32)\n",
        "        size = np.array([IMAGE_SIZE, ceil(0.8 * IMAGE_SIZE)], dtype = np.int32)\n",
        "        w_start = int(floor((1 - 0.8) * IMAGE_SIZE))\n",
        "        w_end = IMAGE_SIZE\n",
        "        h_start = 0\n",
        "        h_end = IMAGE_SIZE\n",
        "    elif index == 2: # Translate top 20 percent\n",
        "        offset = np.array([0.2, 0.0], dtype = np.float32)\n",
        "        size = np.array([ceil(0.8 * IMAGE_SIZE), IMAGE_SIZE], dtype = np.int32)\n",
        "        w_start = 0\n",
        "        w_end = IMAGE_SIZE\n",
        "        h_start = 0\n",
        "        h_end = int(ceil(0.8 * IMAGE_SIZE)) \n",
        "    else: # Translate bottom 20 percent\n",
        "        offset = np.array([-0.2, 0.0], dtype = np.float32)\n",
        "        size = np.array([ceil(0.8 * IMAGE_SIZE), IMAGE_SIZE], dtype = np.int32)\n",
        "        w_start = 0\n",
        "        w_end = IMAGE_SIZE\n",
        "        h_start = int(floor((1 - 0.8) * IMAGE_SIZE))\n",
        "        h_end = IMAGE_SIZE \n",
        "        \n",
        "    return offset, size, w_start, w_end, h_start, h_end\n",
        "\n",
        "def translate_images(X_imgs):\n",
        "    offsets = np.zeros((len(X_imgs), 2), dtype = np.float32)\n",
        "    n_translations = 4\n",
        "    X_translated_arr = []\n",
        "    \n",
        "    tf.reset_default_graph()\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        for i in range(n_translations):\n",
        "            X_translated = np.zeros((len(X_imgs), IMAGE_SIZE, IMAGE_SIZE, 3), \n",
        "\t\t\t\t    dtype = np.float32)\n",
        "            X_translated.fill(1.0) # Filling background color\n",
        "            base_offset, size, w_start, w_end, h_start, h_end = get_translate_parameters(i)\n",
        "            offsets[:, :] = base_offset \n",
        "            glimpses = tf.image.extract_glimpse(X_imgs, size, offsets)\n",
        "            \n",
        "            glimpses = sess.run(glimpses)\n",
        "            X_translated[:, h_start: h_start + size[0], \\\n",
        "\t\t\t w_start: w_start + size[1], :] = glimpses\n",
        "            X_translated_arr.extend(X_translated)\n",
        "    X_translated_arr = np.array(X_translated_arr, dtype = np.float32)\n",
        "    return X_translated_arr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGlBX3D242gY",
        "colab_type": "text"
      },
      "source": [
        "# Step 4.3 - Data rotation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWqRjLsPUic7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rotate_images(X_imgs):\n",
        "    X_rotate = []\n",
        "    tf.reset_default_graph()\n",
        "    X = tf.placeholder(tf.float32, shape = (IMAGE_SIZE, IMAGE_SIZE, 3))\n",
        "    k = tf.placeholder(tf.int32)\n",
        "    tf_img = tf.image.rot90(X, k = k)\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        for img in X_imgs:\n",
        "            for i in range(3):  # Rotation at 90, 180 and 270 degrees\n",
        "                rotated_img = sess.run(tf_img, feed_dict = {X: img, k: i + 1})\n",
        "                X_rotate.append(rotated_img)\n",
        "        \n",
        "    X_rotate = np.array(X_rotate, dtype = np.float32)\n",
        "    return X_rotate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bClG7TG48lr",
        "colab_type": "text"
      },
      "source": [
        "# Step 4.4 - Data Flipping and Transpose"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ii9-b0G3_4dr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def flip_images(X_imgs):\n",
        "    X_flip = []\n",
        "    tf.reset_default_graph()\n",
        "    X = tf.placeholder(tf.float32, shape = (IMAGE_SIZE, IMAGE_SIZE, 3))\n",
        "    tf_img1 = tf.image.flip_left_right(X)\n",
        "    tf_img2 = tf.image.flip_up_down(X)\n",
        "    tf_img3 = tf.image.transpose_image(X)\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        for img in X_imgs:\n",
        "            flipped_imgs = sess.run([tf_img1, tf_img2, tf_img3], feed_dict = {X: img})\n",
        "            X_flip.extend(flipped_imgs)\n",
        "    X_flip = np.array(X_flip, dtype = np.float32)\n",
        "    return X_flip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxJKJDhw5LcZ",
        "colab_type": "text"
      },
      "source": [
        "# Step 5 - Storing data into numpy arrays\n",
        "- More data augmentation techniques will be done in final implementation.\n",
        "- Once data is augmented store them in numpy array and delete them from dynamic memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpHVl7wCCeSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "male = os.listdir('gender_bo/male')\n",
        "male.sort()\n",
        "X_male = np.zeros((len(male),IMAGE_SIZE,IMAGE_SIZE,3))\n",
        "count= 0\n",
        "for i in male:\n",
        "  temp = cv2.imread('gender_bo/male/'+i)\n",
        "  X_male[count,:,:,:] = temp\n",
        "  count+=1\n",
        "\n",
        "male = os.listdir('gender_bo/female')\n",
        "male.sort()\n",
        "X_female = np.zeros((len(male),IMAGE_SIZE,IMAGE_SIZE,3))\n",
        "count= 0\n",
        "for i in male:\n",
        "  temp = cv2.imread('gender_bo/female/'+i)\n",
        "  X_female[count,:,:,:] = temp\n",
        "  count+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd_xLtKmrKXw",
        "colab_type": "text"
      },
      "source": [
        "# Step 5.1 - Scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnGtIglNExW6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaled_imgs_male = central_scale_images(X_male, [0.90, 0.75, 0.80])\n",
        "scaled_imgs_female = central_scale_images(X_female, [0.90, 0.75, 0.80])\n",
        "np.save('scaled_imgs_male',scaled_imgs_male)\n",
        "np.save('scaled_imgs_female',scaled_imgs_female)\n",
        "print(X_male.shape)\n",
        "print(X_female.shape)\n",
        "print(scaled_imgs_male.shape)\n",
        "print(scaled_imgs_female.shape)\n",
        "scaled_imgs_male = None\n",
        "scaled_imgs_female = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK5v2iiJrPw2",
        "colab_type": "text"
      },
      "source": [
        "# Step 5.2 - Scaling + translation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3W46WIJ3E-hS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaled_imgs_male = np.load('scaled_imgs_male.npy')\n",
        "scaled_imgs_female = np.load('scaled_imgs_female.npy')\n",
        "translated_imgs_male = translate_images(X_male)\n",
        "translated_imgs_female = translate_images(X_female)\n",
        "translated_scaled_imgs_male = translate_images(scaled_imgs_male)\n",
        "translated_scaled_imgs_female = translate_images(scaled_imgs_female)\n",
        "np.save('translated_imgs_male',translated_imgs_male)\n",
        "np.save('translated_imgs_female',translated_imgs_female)\n",
        "np.save('translated_scaled_imgs_male',translated_scaled_imgs_male)\n",
        "np.save('translated_scaled_imgs_female',translated_scaled_imgs_female)\n",
        "print(X_male.shape)\n",
        "print(X_female.shape)\n",
        "print(scaled_imgs_male.shape)\n",
        "print(scaled_imgs_female.shape)\n",
        "print(translated_imgs_male.shape)\n",
        "print(translated_imgs_female.shape)\n",
        "print(translated_scaled_imgs_male.shape)\n",
        "print(translated_scaled_imgs_female.shape)\n",
        "scaled_imgs_male = None\n",
        "scaled_imgs_female = None\n",
        "translated_imgs_male = None\n",
        "translated_imgs_female = None\n",
        "translated_scaled_imgs_male = None\n",
        "translated_scaled_imgs_female = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGDoxFGmrVtL",
        "colab_type": "text"
      },
      "source": [
        "# Step 5.3 - Scaling + translation + rotation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZ5mfjxBV05y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaled_imgs_male = np.load('scaled_imgs_male.npy')\n",
        "scaled_imgs_female = np.load('scaled_imgs_female.npy')\n",
        "translated_imgs_male = np.load('translated_imgs_male.npy')\n",
        "translated_imgs_female = np.load('translated_imgs_female.npy')\n",
        "translated_scaled_imgs_male = np.load('translated_scaled_imgs_male.npy')\n",
        "translated_scaled_imgs_female = np.load('translated_scaled_imgs_female.npy')\n",
        "rotated_imgs_male = rotate_images(X_male)\n",
        "rotated_imgs_female = rotate_images(X_female)\n",
        "rotated_scaled_imgs_male = rotate_images(scaled_imgs_male)\n",
        "rotated_scaled_imgs_female = rotate_images(scaled_imgs_female)\n",
        "rotated_translated_imgs_male = rotate_images(translated_imgs_male)\n",
        "rotated_translated_imgs_female = rotate_images(translated_imgs_female)\n",
        "rotated_translated_scaled_imgs_male = rotate_images(translated_scaled_imgs_male)\n",
        "rotated_translated_scaled_imgs_female = rotate_images(translated_scaled_imgs_female)\n",
        "np.save('rotated_imgs_male',rotated_imgs_male)\n",
        "np.save('rotated_imgs_female',rotated_imgs_female)\n",
        "np.save('rotated_scaled_imgs_male',rotated_scaled_imgs_male)\n",
        "np.save('rotated_scaled_imgs_female',rotated_scaled_imgs_female)\n",
        "np.save('rotated_translated_imgs_male',rotated_translated_imgs_male)\n",
        "np.save('rotated_translated_imgs_female',rotated_translated_imgs_female)\n",
        "np.save('rotated_translated_scaled_imgs_male',rotated_translated_scaled_imgs_male)\n",
        "np.save('rotated_translated_scaled_imgs_female',rotated_translated_scaled_imgs_female)\n",
        "print(X_male.shape)\n",
        "print(X_female.shape)\n",
        "print(scaled_imgs_male.shape)\n",
        "print(scaled_imgs_female.shape)\n",
        "print(translated_imgs_male.shape)\n",
        "print(translated_imgs_female.shape)\n",
        "print(translated_scaled_imgs_male.shape)\n",
        "print(translated_scaled_imgs_female.shape)\n",
        "print(rotated_imgs_male.shape)\n",
        "print(rotated_imgs_female.shape)\n",
        "print(rotated_scaled_imgs_male.shape)\n",
        "print(rotated_scaled_imgs_female.shape)\n",
        "print(rotated_translated_imgs_male.shape)\n",
        "print(rotated_translated_imgs_female.shape)\n",
        "print(rotated_translated_scaled_imgs_male.shape)\n",
        "print(rotated_translated_scaled_imgs_female.shape)\n",
        "scaled_imgs_male = None\n",
        "scaled_imgs_female = None\n",
        "translated_imgs_male = None\n",
        "translated_imgs_female = None\n",
        "translated_scaled_imgs_male = None\n",
        "translated_scaled_imgs_female = None\n",
        "\n",
        "rotated_imgs_male = None\n",
        "rotated_imgs_female = None\n",
        "rotated_scaled_imgs_male = None\n",
        "rotated_scaled_imgs_female = None\n",
        "rotated_translated_imgs_male = None\n",
        "rotated_translated_imgs_female = None\n",
        "rotated_translated_scaled_imgs_male = None\n",
        "rotated_translated_scaled_imgs_female = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxYsXDxlra_r",
        "colab_type": "text"
      },
      "source": [
        "# Step 5.4 - Scaling + translation + rotation + (flipping + transpose)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OKCpFpKeSKR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaled_imgs_male = np.load('scaled_imgs_male.npy')\n",
        "scaled_imgs_female = np.load('scaled_imgs_female.npy')\n",
        "rotated_imgs_male = np.load('rotated_imgs_male.npy')\n",
        "rotated_imgs_female = np.load('rotated_imgs_female.npy')\n",
        "translated_imgs_male = np.load('translated_imgs_male.npy')\n",
        "translated_imgs_female = np.load('translated_imgs_female.npy')\n",
        "translated_scaled_imgs_male = np.load('translated_scaled_imgs_male.npy')\n",
        "translated_scaled_imgs_female = np.load('translated_scaled_imgs_female.npy')\n",
        "rotated_scaled_imgs_male = np.load('rotated_scaled_imgs_male.npy')\n",
        "rotated_scaled_imgs_female = np.load('rotated_scaled_imgs_female.npy')\n",
        "rotated_translated_imgs_male = np.load('rotated_translated_imgs_male.npy')\n",
        "rotated_translated_imgs_female = np.load('rotated_translated_imgs_female.npy')\n",
        "rotated_translated_scaled_imgs_male = np.load('rotated_translated_scaled_imgs_male.npy')\n",
        "rotated_translated_scaled_imgs_female = np.load('rotated_translated_scaled_imgs_female.npy')\n",
        "\n",
        "\n",
        "flipped_imgs_male = flip_images(X_male)\n",
        "flipped_imgs_female = flip_images(X_female)\n",
        "flipped_scaled_imgs_male = flip_images(scaled_imgs_male)\n",
        "flipped_scaled_imgs_female = flip_images(scaled_imgs_female)\n",
        "flipped_rotated_imgs_male = flip_images(rotated_imgs_male)\n",
        "flipped_rotated_imgs_female = flip_images(rotated_imgs_female)\n",
        "flipped_translated_imgs_male = flip_images(translated_imgs_male)\n",
        "flipped_translated_imgs_female = flip_images(translated_imgs_female)\n",
        "flipped_translated_scaled_imgs_male = flip_images(translated_scaled_imgs_male)\n",
        "flipped_translated_scaled_imgs_female = flip_images(translated_scaled_imgs_female)\n",
        "flipped_rotated_scaled_imgs_male = flip_images(rotated_scaled_imgs_male)\n",
        "flipped_rotated_scaled_imgs_female = flip_images(rotated_scaled_imgs_female)\n",
        "flipped_rotated_translated_imgs_male = flip_images(rotated_translated_imgs_male)\n",
        "flipped_rotated_translated_imgs_female = flip_images(rotated_translated_imgs_female)\n",
        "flipped_rotated_translated_scaled_imgs_male = flip_images(rotated_translated_scaled_imgs_male)\n",
        "flipped_rotated_translated_scaled_imgs_female = flip_images(rotated_translated_scaled_imgs_female)\n",
        "\n",
        "np.save('flipped_imgs_male',flipped_imgs_male)\n",
        "np.save('flipped_imgs_female',flipped_imgs_female)\n",
        "np.save('flipped_scaled_imgs_male',flipped_scaled_imgs_male)\n",
        "np.save('flipped_scaled_imgs_female',flipped_scaled_imgs_female)\n",
        "np.save('flipped_rotated_imgs_male',flipped_rotated_imgs_male)\n",
        "np.save('flipped_rotated_imgs_female',flipped_rotated_imgs_female)\n",
        "np.save('flipped_translated_imgs_male',flipped_translated_imgs_male)\n",
        "np.save('flipped_translated_imgs_female',flipped_translated_imgs_female)\n",
        "np.save('flipped_translated_scaled_imgs_male',flipped_translated_scaled_imgs_male)\n",
        "np.save('flipped_translated_scaled_imgs_female',flipped_translated_scaled_imgs_female)\n",
        "np.save('flipped_rotated_scaled_imgs_male',flipped_rotated_scaled_imgs_male)\n",
        "np.save('flipped_rotated_scaled_imgs_female',flipped_rotated_scaled_imgs_female)\n",
        "np.save('flipped_rotated_translated_imgs_male',flipped_rotated_translated_imgs_male)\n",
        "np.save('flipped_rotated_translated_imgs_female',flipped_rotated_translated_imgs_female)\n",
        "np.save('flipped_rotated_translated_scaled_imgs_male',flipped_rotated_translated_scaled_imgs_male)\n",
        "np.save('flipped_rotated_translated_scaled_imgs_female',flipped_rotated_translated_scaled_imgs_female)\n",
        "\n",
        "\n",
        "print(X_male.shape)\n",
        "print(X_female.shape)\n",
        "print(scaled_imgs_male.shape)\n",
        "print(scaled_imgs_female.shape)\n",
        "print(translated_imgs_male.shape)\n",
        "print(translated_imgs_female.shape)\n",
        "print(rotated_imgs_male.shape)\n",
        "print(rotated_imgs_female.shape)\n",
        "print(flipped_imgs_male.shape)\n",
        "print(flipped_imgs_female.shape)\n",
        "print(translated_scaled_imgs_male.shape)\n",
        "print(translated_scaled_imgs_female.shape)\n",
        "print(rotated_scaled_imgs_male.shape)\n",
        "print(rotated_scaled_imgs_female.shape)\n",
        "print(rotated_translated_imgs_male.shape)\n",
        "print(rotated_translated_imgs_female.shape)\n",
        "print(flipped_scaled_imgs_male.shape)\n",
        "print(flipped_scaled_imgs_female.shape)\n",
        "print(flipped_rotated_imgs_male.shape)\n",
        "print(flipped_rotated_imgs_female.shape)\n",
        "print(flipped_translated_imgs_male.shape)\n",
        "print(flipped_translated_imgs_female.shape)\n",
        "print(rotated_translated_scaled_imgs_male.shape)\n",
        "print(rotated_translated_scaled_imgs_female.shape)\n",
        "print(flipped_translated_scaled_imgs_male.shape)\n",
        "print(flipped_translated_scaled_imgs_female.shape)\n",
        "print(flipped_rotated_scaled_imgs_male.shape)\n",
        "print(flipped_rotated_scaled_imgs_female.shape)\n",
        "print(flipped_rotated_translated_imgs_male.shape)\n",
        "print(flipped_rotated_translated_imgs_female.shape)\n",
        "print(flipped_rotated_translated_scaled_imgs_male.shape)\n",
        "print(flipped_rotated_translated_scaled_imgs_female.shape)\n",
        "\n",
        "scaled_imgs_male = None\n",
        "scaled_imgs_female = None\n",
        "flipped_imgs_male = None\n",
        "flipped_imgs_female = None\n",
        "translated_imgs_male = None\n",
        "translated_imgs_female = None\n",
        "translated_scaled_imgs_male = None\n",
        "translated_scaled_imgs_female = None\n",
        "rotated_imgs_male = None\n",
        "rotated_imgs_female = None\n",
        "rotated_scaled_imgs_male = None\n",
        "rotated_scaled_imgs_female = None\n",
        "rotated_translated_imgs_male = None\n",
        "rotated_translated_imgs_female = None\n",
        "flipped_scaled_imgs_male = None\n",
        "flipped_scaled_imgs_female = None\n",
        "flipped_rotated_imgs_male = None\n",
        "flipped_rotated_imgs_female = None\n",
        "flipped_translated_imgs_male = None\n",
        "flipped_translated_imgs_female = None\n",
        "rotated_translated_scaled_imgs_male = None\n",
        "rotated_translated_scaled_imgs_female = None\n",
        "flipped_translated_scaled_imgs_male = None\n",
        "flipped_translated_scaled_imgs_female = None\n",
        "flipped_rotated_scaled_imgs_male = None\n",
        "flipped_rotated_scaled_imgs_female = None\n",
        "flipped_rotated_translated_imgs_male = None\n",
        "flipped_rotated_translated_imgs_female = None\n",
        "flipped_rotated_translated_scaled_imgs_male = None\n",
        "flipped_rotated_translated_scaled_imgs_female = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGHBuWRD7mho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrkHYYf455El",
        "colab_type": "text"
      },
      "source": [
        "# Step 6 - Load the augmented Data\n",
        "- Diffrent kind of data augmented networks will be trained and ensemmble of them will be used in final implementation. This is just one of them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxQL4GHI8dZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('data')\n",
        "os.mkdir('data/train')\n",
        "os.mkdir('data/train/male')\n",
        "os.mkdir('data/train/female')\n",
        "\n",
        "os.mkdir('data/validation')\n",
        "os.mkdir('data/validation/male')\n",
        "os.mkdir('data/validation/female')\n",
        "os.mkdir('models')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZR6w8ii800K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = np.load('flipped_rotated_scaled_imgs_male.npy')\n",
        "b = np.load('flipped_rotated_scaled_imgs_female.npy')\n",
        "for i,j in enumerate(a):\n",
        "  cv2.imwrite('data/train/male/'+str(i)+'.jpg',j)\n",
        "for i,j in enumerate(b):\n",
        "  cv2.imwrite('data/train/female/'+str(i)+'.jpg',j)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-v5uDdj9VpZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = X_male\n",
        "b = X_female\n",
        "for i,j in enumerate(a):\n",
        "  cv2.imwrite('data/validation/male/'+str(i)+'.jpg',j)\n",
        "for i,j in enumerate(b):\n",
        "  cv2.imwrite('data/validation/female/'+str(i)+'.jpg',j)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csvUHZ696RMH",
        "colab_type": "text"
      },
      "source": [
        "# Step 7 - Basic Imports and datagenrator.\n",
        "- Do all the required imports and for **keras** use it's image generator for passing data into model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpuqqvSo-8TN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
        "from keras import optimizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "id0rdA_N_VHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dimensions of our images.\n",
        "img_width, img_height = IMAGE_SIZE, IMAGE_SIZE\n",
        "\n",
        "train_data_dir = 'data/train'\n",
        "validation_data_dir = 'data/validation'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTaDud_5_j8C",
        "colab_type": "code",
        "outputId": "e383ffbd-712e-4443-e3a9-e2166868e239",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# used to rescale the pixel values from [0, 255] to [0, 1] interval\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# automagically retrieve images and their classes for train and validation sets\n",
        "train_generator = datagen.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=16,\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_generator = datagen.flow_from_directory(\n",
        "        validation_data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=32,\n",
        "        class_mode='binary')#change the batch size"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 5319 images belonging to 2 classes.\n",
            "Found 197 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VP9d69696l81",
        "colab_type": "text"
      },
      "source": [
        "# Step 8 - Declare small model to trained from start\n",
        "- Use Maxpool to decrease size instead of playing with **stride**.\n",
        "- At the end use **sigmoid** as gender classification is binary else for other classification use **softmax**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6hRLUnn_wDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Convolution2D(32, 3, 3, input_shape=(img_width, img_height,3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Convolution2D(32, 3, 3))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Convolution2D(64, 3, 3))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qV2tJt1y7MH4",
        "colab_type": "text"
      },
      "source": [
        "# Step 9 - Compiling\n",
        "- Compile the model with RMSprop optimizer instead of SGD as it is shown to work much better at small dataset \n",
        "- Use accuracy of the network the judgement parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOtkEgeI_7IT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Lyxs9-i7uss",
        "colab_type": "text"
      },
      "source": [
        "# Step 10 - Declare training params\n",
        "- No. of epochs\n",
        "- No. of training samples\n",
        "- No. of validation samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SLxFn3LAEHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_epoch = 150\n",
        "nb_train_samples = 5319\n",
        "nb_validation_samples = 197"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnkjkjUj7iYp",
        "colab_type": "text"
      },
      "source": [
        "# Step 11 - Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjiAZvDoAKqc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit_generator(\n",
        "        train_generator,\n",
        "        samples_per_epoch=nb_train_samples,\n",
        "        nb_epoch=nb_epoch,\n",
        "        validation_data=validation_generator,\n",
        "        nb_val_samples=nb_validation_samples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC_acXLj8DVG",
        "colab_type": "text"
      },
      "source": [
        "# Step 12- Evaluate performance of trained network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92vZvLWYASaK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.evaluate_generator(validation_generator, nb_validation_samples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2QuOJXa8LlG",
        "colab_type": "text"
      },
      "source": [
        "# Step 13 - Save the trained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMyWGV7qBKq3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights('models/basic_cnn_150_epochs.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgyvaMJR8V1J",
        "colab_type": "text"
      },
      "source": [
        "# Step 14 - Transfer Learning\n",
        "- Using VGG19 for transfer learning.\n",
        "- Remake the **fully connected** layers of the network as per choice.\n",
        "- Can use early checkpoint but not used for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzWE5DWgI5fq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import applications\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential, Model \n",
        "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
        "from keras import backend as k \n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "img_width, img_height = 256, 256\n",
        "train_data_dir = \"data/train\"\n",
        "validation_data_dir = \"data/validation\"\n",
        "nb_train_samples = 5319 \n",
        "nb_validation_samples = 197\n",
        "batch_size = 32\n",
        "epochs = 100\n",
        "\n",
        "model = applications.VGG19(weights = \"imagenet\", include_top=False, input_shape = (img_width, img_height, 3))\n",
        "\n",
        "\n",
        "# Freeze the layers which you don't want to train. Here I am freezing the first 17 layers.\n",
        "for layer in model.layers[:17]:\n",
        "    layer.trainable = False\n",
        "\n",
        "#Adding custom Layers \n",
        "x = model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(1024, activation=\"relu\")(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(256, activation=\"relu\")(x)\n",
        "predictions = Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "# creating the final model \n",
        "model_final = Model(input = model.input, output = predictions)\n",
        "\n",
        "# compile the model (can use SGD or RMSprop)\n",
        "#model_final.compile(loss = \"binary_crossentropy\", optimizer = optimizers.SGD(lr=0.0001, momentum=0.9), metrics=[\"accuracy\"])\n",
        "model_final.compile(loss = \"binary_crossentropy\", optimizer = optimizers.RMSprop(lr=0.0001), metrics=[\"accuracy\"])\n",
        "\n",
        "# Initiate the train and test generators with data_genrator\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "train_generator = datagen.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=16,\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_generator = datagen.flow_from_directory(\n",
        "        validation_data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=16,\n",
        "        class_mode='binary')#change the batch size\n",
        "\n",
        "# Save the model according to the conditions  \n",
        "checkpoint = ModelCheckpoint(\"vgg16_1.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "#early = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')\n",
        "\n",
        "\n",
        "# Train the model \n",
        "model_final.fit_generator(\n",
        "train_generator,\n",
        "samples_per_epoch = nb_train_samples,\n",
        "epochs = epochs,\n",
        "validation_data = validation_generator,\n",
        "nb_val_samples = nb_validation_samples,\n",
        "callbacks = [checkpoint])\n",
        "#callbacks = [checkpoint, early])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6_RabMqqF2I",
        "colab_type": "text"
      },
      "source": [
        "# Step 14 - Custom Model\n",
        "- Similar to VGG19.\n",
        "- Used same architecture as VGG but less convolution layer in each block.\n",
        "- Less no. of blocks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zmEpollY5nT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Convolution2D(32, 3, 3, border_mode='same',input_shape=(img_width, img_height,3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization(axis=-1))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Convolution2D(64, 3, 3, border_mode='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization(axis=-1))\n",
        "model.add(Convolution2D(64, 3, 3, border_mode='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization(axis=-1))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Convolution2D(128, 3, 3, border_mode='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization(axis=-1))\n",
        "model.add(Convolution2D(128, 3, 3, border_mode='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization(axis=-1))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOFuocEkeO6G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}